{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103b62a-b720-4bcc-a931-cdd83b1f3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 1: Imports and Global Configurations\n",
    "#############################################\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ed298-b253-420a-8200-b9336f3bf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 2: Custom Dataset & Transforms\n",
    "#############################################\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for Chest X-Ray segmentation.\n",
    "    Assumes each image has a corresponding mask with the same file name \n",
    "    in a 'masks' folder (all PNG).\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_paths = sorted(glob.glob(os.path.join(images_dir, \"*.png\")))\n",
    "        self.mask_paths = sorted(glob.glob(os.path.join(masks_dir, \"*.png\")))\n",
    "        \n",
    "        assert len(self.image_paths) == len(self.mask_paths), \\\n",
    "            \"Number of images and masks do not match.\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # grayscale mask\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = {\"image\": image, \"mask\": mask}\n",
    "            sample = self.transform(sample)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "        else:\n",
    "            # Default: just convert to tensor (no resize, no normalization)\n",
    "            image = T.ToTensor()(image)\n",
    "            mask = T.ToTensor()(mask)\n",
    "            mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class JointTransformWrapper:\n",
    "    \"\"\"\n",
    "    A wrapper to apply transforms that require both image and mask \n",
    "    simultaneously (e.g. resize, random flips, etc.).\n",
    "    \"\"\"\n",
    "    def __init__(self, augment=True, image_size=(224, 224)):\n",
    "        self.augment = augment\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Common transformations (resize, etc.)\n",
    "        self.common_transforms = T.Compose([\n",
    "            T.Resize(self.image_size),\n",
    "        ])\n",
    "        \n",
    "        # Augmentations (e.g. random horizontal flip)\n",
    "        self.augment_transform = T.RandomHorizontalFlip(p=0.5)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        self.to_tensor_img = T.ToTensor()\n",
    "        self.to_tensor_mask = T.ToTensor()\n",
    "\n",
    "        # ImageNet normalization (recommended for pretrained Swin)\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "        \n",
    "        # 1. Resize\n",
    "        image = self.common_transforms(image)\n",
    "        mask = self.common_transforms(mask)\n",
    "        \n",
    "        if self.augment:\n",
    "            # Ensure the same augmentation (e.g. flip) is applied to both image & mask\n",
    "            seed = np.random.randint(2147483647)\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.augment_transform(image)\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.augment_transform(mask)\n",
    "        \n",
    "        # 2. To Tensor\n",
    "        image = self.to_tensor_img(image)\n",
    "        mask = self.to_tensor_mask(mask)\n",
    "        \n",
    "        # 3. Normalize image (mask is 0/1, so no normalization)\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        # 4. Binarize mask\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return {\"image\": image, \"mask\": mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a4617-2ed0-4956-ad42-e32bfa490e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 3: Create Dataset Instances & Splits\n",
    "#############################################\n",
    "\n",
    "# Dataset Path\n",
    "dataset_path = r\"C:\\Users\\offic\\OneDrive\\Masaüstü\\datasets\\Chest_XRay\"\n",
    "images_dir = os.path.join(dataset_path, \"images\")\n",
    "masks_dir = os.path.join(dataset_path, \"masks\")\n",
    "\n",
    "# Create the transformation pipeline\n",
    "joint_transform = JointTransformWrapper(augment=True, image_size=(224, 224))\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = ChestXRayDataset(images_dir, masks_dir, transform=joint_transform)\n",
    "print(\"Total samples in dataset:\", len(full_dataset))\n",
    "\n",
    "# Split into train/val/test\n",
    "dataset_len = len(full_dataset)\n",
    "train_size = int(0.7 * dataset_len)  # 70%\n",
    "val_size = int(0.15 * dataset_len)   # 15%\n",
    "test_size = dataset_len - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"Val:\", len(val_dataset), \"Test:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7078461-7943-4a53-81f1-726d63f3052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 4: Create DataLoaders\n",
    "#############################################\n",
    "\n",
    "def create_dataloaders(train_ds, val_ds, test_ds, batch_size=4):\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "batch_size = 4\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902e984-8f9c-4431-b88b-b3681117d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 5: Define Simpler Decoder (No Attention)\n",
    "#         + SwinTransformerSegModel\n",
    "#############################################\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simpler UNet-like decoder WITHOUT attention blocks to reduce computation.\n",
    "    We'll keep skip connections, transposed convolutions, \n",
    "    and final upsample to get 224x224 output.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_channels, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # e.g. encoder_channels = [96, 192, 384, 768] for swin_tiny\n",
    "        # We'll do top-down upsampling with skip connections.\n",
    "        \n",
    "        self.conv_f4 = nn.Conv2d(encoder_channels[3], 512, kernel_size=1)\n",
    "        self.conv_f3 = nn.Conv2d(encoder_channels[2], 256, kernel_size=1)\n",
    "        self.conv_f2 = nn.Conv2d(encoder_channels[1], 128, kernel_size=1)\n",
    "        self.conv_f1 = nn.Conv2d(encoder_channels[0], 64,  kernel_size=1)\n",
    "        \n",
    "        # Decoder up stages\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # fuse with f3\n",
    "        self.fuse1 = nn.Sequential(\n",
    "            nn.Conv2d(256+256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # fuse with f2\n",
    "        self.fuse2 = nn.Sequential(\n",
    "            nn.Conv2d(128+128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # fuse with f1\n",
    "        self.fuse3 = nn.Sequential(\n",
    "            nn.Conv2d(64+64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # An optional final upsample step to ensure 224x224 \n",
    "        # (depending on your input resolution & Swin specifics).\n",
    "        self.up5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        # features = [f1, f2, f3, f4]\n",
    "        f1, f2, f3, f4 = features\n",
    "        \n",
    "        # 1x1 conv to unify channel sizes\n",
    "        f4 = self.conv_f4(f4)\n",
    "        f3 = self.conv_f3(f3)\n",
    "        f2 = self.conv_f2(f2)\n",
    "        f1 = self.conv_f1(f1)\n",
    "        \n",
    "        # Stage 1\n",
    "        x = f4\n",
    "        x = self.up1(x)                     # upsample from f4\n",
    "        x = self.fuse1(torch.cat([x, f3], dim=1))\n",
    "        \n",
    "        # Stage 2\n",
    "        x = self.up2(x)\n",
    "        x = self.fuse2(torch.cat([x, f2], dim=1))\n",
    "        \n",
    "        # Stage 3\n",
    "        x = self.up3(x)\n",
    "        x = self.fuse3(torch.cat([x, f1], dim=1))\n",
    "        \n",
    "        # Stage 4\n",
    "        x = self.up4(x)\n",
    "        \n",
    "        # Stage 5 (final up to 224x224, if needed)\n",
    "        x = self.up5(x)\n",
    "        \n",
    "        # Final conv\n",
    "        x = self.out_conv(x)  # (B, out_channels, H, W)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerSegModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full segmentation model with a Swin Transformer encoder\n",
    "    and our simpler UNet-like decoder (no attention).\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name=\"swin_tiny_patch4_window7_224\", out_channels=1):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(backbone_name, pretrained=True, features_only=True)\n",
    "        encoder_channels = self.encoder.feature_info.channels()  # e.g. [96, 192, 384, 768]\n",
    "        self.decoder = SimpleDecoder(encoder_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from the Swin encoder\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # If channels-last, ensure channels-first\n",
    "        permuted_features = []\n",
    "        for f in features:\n",
    "            if f.dim() == 4 and f.shape[1] < f.shape[-1]:\n",
    "                f = f.permute(0, 3, 1, 2)\n",
    "            permuted_features.append(f)\n",
    "        \n",
    "        # Pass to decoder\n",
    "        seg_map = self.decoder(permuted_features)\n",
    "        return seg_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c0a09-6345-46b9-aea9-ed7143228d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 6: Loss Functions & Metrics\n",
    "#############################################\n",
    "\n",
    "def dice_loss(pred, target, smooth=1e-5):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    dice = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted combination of BCEWithLogitsLoss and Dice Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_bce=0.5):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.weight_bce = weight_bce\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        loss_bce = self.bce(pred, target)\n",
    "        loss_dice = dice_loss(pred, target)\n",
    "        return self.weight_bce * loss_bce + (1 - self.weight_bce) * loss_dice\n",
    "\n",
    "def dice_coefficient(pred, target, threshold=0.5, smooth=1e-5):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean().item()\n",
    "\n",
    "def iou_coefficient(pred, target, threshold=0.5, smooth=1e-5):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.mean().item()\n",
    "\n",
    "def compute_confusion_matrix(pred, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes pixel-level confusion matrix (TP, FP, TN, FN) for a batch.\n",
    "    pred, target: B x 1 x H x W (tensors)\n",
    "    Returns: TP, FP, TN, FN (scalars)\n",
    "    \"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > threshold).float()\n",
    "    \n",
    "    # Flatten\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    \n",
    "    tp = (pred_flat * target_flat).sum()\n",
    "    fp = (pred_flat * (1 - target_flat)).sum()\n",
    "    fn = ((1 - pred_flat) * target_flat).sum()\n",
    "    tn = ((1 - pred_flat) * (1 - target_flat)).sum()\n",
    "    \n",
    "    return tp.item(), fp.item(), tn.item(), fn.item()\n",
    "\n",
    "def compute_additional_metrics(tp, fp, tn, fn, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Given total TP, FP, TN, FN, compute additional metrics:\n",
    "    accuracy, precision, recall, specificity, f1\n",
    "    \"\"\"\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    specificity = tn / (tn + fp + eps)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1_score\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426ea24-8c69-4ccf-a79f-dfda22eb9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 7: Training Loop with Early Stopping,\n",
    "#         LR Scheduling + Extended Metrics,\n",
    "#         Time Tracking (elapsed/remaining).\n",
    "#############################################\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    running_iou  = 0.0\n",
    "    \n",
    "    # For confusion matrix\n",
    "    total_tp, total_fp, total_tn, total_fn = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # dice & iou\n",
    "            d = dice_coefficient(outputs, masks)\n",
    "            i = iou_coefficient(outputs, masks)\n",
    "            running_dice += d * images.size(0)\n",
    "            running_iou  += i * images.size(0)\n",
    "            \n",
    "            # confusion matrix\n",
    "            tp, fp, tn, fn = compute_confusion_matrix(outputs, masks)\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_tn += tn\n",
    "            total_fn += fn\n",
    "    \n",
    "    epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_val_dice = running_dice / len(val_loader.dataset)\n",
    "    epoch_val_iou  = running_iou  / len(val_loader.dataset)\n",
    "    \n",
    "    # Additional metrics\n",
    "    metrics = compute_additional_metrics(total_tp, total_fp, total_tn, total_fn)\n",
    "    \n",
    "    return epoch_val_loss, epoch_val_dice, epoch_val_iou, metrics\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\" Convert seconds to hh:mm:ss string. \"\"\"\n",
    "    import datetime\n",
    "    return str(datetime.timedelta(seconds=int(seconds)))\n",
    "\n",
    "\n",
    "def train_model(model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                device, \n",
    "                epochs=20, \n",
    "                patience=5, \n",
    "                lr=1e-4, \n",
    "                weight_bce=0.5,\n",
    "                reduce_on_plateau=True):\n",
    "    \"\"\"\n",
    "    Trains the model with:\n",
    "    - ComboLoss (BCE + Dice)\n",
    "    - Early stopping on validation loss\n",
    "    - (Optionally) LR scheduling (ReduceLROnPlateau)\n",
    "    - Time tracking: prints elapsed & estimated remaining time each epoch\n",
    "    Returns the best model (by val loss) and a training history.\n",
    "    \"\"\"\n",
    "    criterion = ComboLoss(weight_bce=weight_bce)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Learning Rate Scheduler (Reduce LR on Plateau) - no verbose param\n",
    "    if reduce_on_plateau:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=2\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses   = []\n",
    "    val_dices    = []\n",
    "    val_ious     = []\n",
    "    \n",
    "    # We'll also store additional metrics\n",
    "    val_accuracies   = []\n",
    "    val_precisions   = []\n",
    "    val_recalls      = []\n",
    "    val_specificities= []\n",
    "    val_f1s          = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    no_improve_count = 0\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_dice, val_iou, additional_metrics = validate_one_epoch(model, val_loader, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_dices.append(val_dice)\n",
    "        val_ious.append(val_iou)\n",
    "        val_accuracies.append(additional_metrics[\"accuracy\"])\n",
    "        val_precisions.append(additional_metrics[\"precision\"])\n",
    "        val_recalls.append(additional_metrics[\"recall\"])\n",
    "        val_specificities.append(additional_metrics[\"specificity\"])\n",
    "        val_f1s.append(additional_metrics[\"f1_score\"])\n",
    "        \n",
    "        # Scheduler step (on val loss)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        else:\n",
    "            current_lr = lr\n",
    "        \n",
    "        # Time calculations\n",
    "        epoch_end = time.time()\n",
    "        epoch_time = epoch_end - epoch_start\n",
    "        total_time = epoch_end - start_time\n",
    "        avg_epoch_time = total_time / epoch\n",
    "        remaining_time = avg_epoch_time * (epochs - epoch)\n",
    "        \n",
    "        print(f\"Epoch [{epoch}/{epochs}] (LR: {current_lr:.6f})\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Dice: {val_dice:.4f} | IoU: {val_iou:.4f}\")\n",
    "        print(f\"  Accuracy: {additional_metrics['accuracy']:.4f}, \"\n",
    "              f\"Precision: {additional_metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {additional_metrics['recall']:.4f}, \"\n",
    "              f\"Specificity: {additional_metrics['specificity']:.4f}, \"\n",
    "              f\"F1: {additional_metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        print(f\"  Time Elapsed: {format_time(total_time)} | \"\n",
    "              f\"Epoch Time: {format_time(epoch_time)} | \"\n",
    "              f\"Est. Remaining: {format_time(remaining_time)}\\n\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if no_improve_count >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"val_dice\": val_dices,\n",
    "        \"val_iou\": val_ious,\n",
    "        \"val_accuracy\": val_accuracies,\n",
    "        \"val_precision\": val_precisions,\n",
    "        \"val_recall\": val_recalls,\n",
    "        \"val_specificity\": val_specificities,\n",
    "        \"val_f1\": val_f1s\n",
    "    }\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7bbe4-9592-4578-9e04-69f729abee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 8: Train the Model\n",
    "#############################################\n",
    "\n",
    "model = SwinTransformerSegModel(\n",
    "    backbone_name=\"swin_tiny_patch4_window7_224\",\n",
    "    out_channels=1\n",
    ").to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 20\n",
    "patience = 5\n",
    "lr = 1e-4\n",
    "weight_bce = 0.5\n",
    "\n",
    "model, history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    lr=lr,\n",
    "    weight_bce=weight_bce,\n",
    "    reduce_on_plateau=True  # LR scheduling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12f771-2f1f-45da-913d-690e8a78017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 9: Plot Training Curves\n",
    "#############################################\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs_range, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs_range, history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs_range, history[\"val_dice\"], label=\"Val Dice\")\n",
    "    plt.title(\"Dice\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Dice\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs_range, history[\"val_iou\"], label=\"Val IoU\")\n",
    "    plt.title(\"IoU\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"IoU\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs_range, history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epochs_range, history[\"val_precision\"], label=\"Val Precision\")\n",
    "    plt.plot(epochs_range, history[\"val_recall\"], label=\"Val Recall\")\n",
    "    plt.title(\"Precision & Recall\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(epochs_range, history[\"val_specificity\"], label=\"Val Specificity\")\n",
    "    plt.plot(epochs_range, history[\"val_f1\"], label=\"Val F1\")\n",
    "    plt.title(\"Specificity & F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709e78b-0e58-42f1-b393-c4e5c1be4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 10: Final Evaluation on Test Set \n",
    "#          (Multiple metrics)\n",
    "#############################################\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    criterion = ComboLoss()  # same combo used in training\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_dice = 0.0\n",
    "    test_iou  = 0.0\n",
    "    \n",
    "    total_tp, total_fp, total_tn, total_fn = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            d = dice_coefficient(outputs, masks)\n",
    "            i = iou_coefficient(outputs, masks)\n",
    "            test_dice += d * images.size(0)\n",
    "            test_iou  += i * images.size(0)\n",
    "            \n",
    "            tp, fp, tn, fn = compute_confusion_matrix(outputs, masks)\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_tn += tn\n",
    "            total_fn += fn\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_dice /= len(test_loader.dataset)\n",
    "    test_iou  /= len(test_loader.dataset)\n",
    "    \n",
    "    additional_metrics = compute_additional_metrics(total_tp, total_fp, total_tn, total_fn)\n",
    "    \n",
    "    print(\"=== Test Results ===\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Dice: {test_dice:.4f} | IoU: {test_iou:.4f}\")\n",
    "    print(f\"Accuracy: {additional_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {additional_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {additional_metrics['recall']:.4f}\")\n",
    "    print(f\"Specificity: {additional_metrics['specificity']:.4f}\")\n",
    "    print(f\"F1: {additional_metrics['f1_score']:.4f}\")\n",
    "\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b1ca5-3e73-47cf-801a-86730e7f5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Cell 11: Overlay Predictions for Visualization\n",
    "#############################################\n",
    "\n",
    "def overlay_mask_on_image(image, mask, alpha=0.5, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Overlays a binary mask on an image (both as NumPy arrays).\n",
    "    image: (H x W x 3), RGB or BGR\n",
    "    mask:  (H x W), 0/1\n",
    "    alpha: blending factor\n",
    "    color: color for the mask overlay\n",
    "    \"\"\"\n",
    "    overlay = image.copy()\n",
    "    overlay[mask > 0] = color\n",
    "    return cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n",
    "\n",
    "\n",
    "def visualize_predictions(model, loader, num_samples=4):\n",
    "    model.eval()\n",
    "    \n",
    "    batch = next(iter(loader))  # get one batch\n",
    "    images, masks = batch\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs)\n",
    "        preds = (preds > 0.5).float()\n",
    "    \n",
    "    for i in range(min(num_samples, images.size(0))):\n",
    "        # Convert to CPU numpy\n",
    "        img_np = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "        mask_gt = masks[i].cpu().numpy().squeeze()\n",
    "        mask_pred = preds[i].cpu().numpy().squeeze()\n",
    "        \n",
    "        # Denormalize image (if you used ImageNet stats)\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std  = np.array([0.229, 0.224, 0.225])\n",
    "        img_np = (img_np * std + mean)\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        img_np = (img_np * 255).astype(np.uint8)\n",
    "        \n",
    "        # Overlay ground truth (in RED for example)\n",
    "        overlay_gt   = overlay_mask_on_image(img_np, mask_gt, alpha=0.5, color=(255, 0, 0))\n",
    "        # Overlay prediction (in GREEN)\n",
    "        overlay_pred = overlay_mask_on_image(img_np, mask_pred, alpha=0.5, color=(0, 255, 0))\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "        \n",
    "        axes[0].imshow(img_np)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        \n",
    "        axes[1].imshow(overlay_gt)\n",
    "        axes[1].set_title(\"Ground Truth Overlay (Red)\")\n",
    "        axes[1].axis(\"off\")\n",
    "        \n",
    "        axes[2].imshow(overlay_pred)\n",
    "        axes[2].set_title(\"Prediction Overlay (Green)\")\n",
    "        axes[2].axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize a few predictions from the test set\n",
    "visualize_predictions(model, test_loader, num_samples=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
